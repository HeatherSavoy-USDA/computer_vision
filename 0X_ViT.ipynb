{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Bonus Notebook\n",
    "\n",
    "This notebook is a bonus notebook that demonstrates how to use the Vision Transformer (ViT) model for image classification. The dataset used is the [Fruits Detection dataset](https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/data) from Kaggle. The dataset contains images of fruits and their bounding boxes. The goal is to classify the fruits in the images.\n",
    "\n",
    "As before, the dataset was found on. [Check out the dataset information](https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/data)\n",
    "\n",
    "<img src=\"notebook_images/fruits_detection_dataset-cover.jpg\" \n",
    "        alt=\"Image of fruits and bounding boxes from the dataset cover image\" \n",
    "        width=\"1000\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "This notebook will use the `ViT` model from the `transformers` library. The `transformers` library is a library that provides state-of-the-art models for Natural Language Processing (NLP) and Computer Vision (CV). The `ViT` model is a model that uses the Vision Transformer architecture for image classification tasks. This framework is developed by Hugging Face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torchsummary\n",
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing supporting libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Importing the necessary libraries for the dataset\n",
    "import numpy as np # NumPy\n",
    "from PIL import Image # Python Imaging Library\n",
    "import pandas as pd # Pandas\n",
    "from sklearn.model_selection import train_test_split # Train-Test Split\n",
    "from sklearn.preprocessing import StandardScaler # Standard Scaler\n",
    "from sklearn.datasets import fetch_openml # Fetch OpenML\n",
    "from sklearn.metrics import accuracy_score # Accuracy Score\n",
    "\n",
    "# Importing the necessary libraries for the visualization\n",
    "import matplotlib.pyplot as plt # Matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Importing the necessary libraries for ViT\n",
    "import torch # PyTorch\n",
    "import torch.nn as nn # Neural Network Module\n",
    "import torch.nn.functional as F # Functional Module that contains activation functions\n",
    "import torchvision # TorchVision\n",
    "from einops import rearrange # Rearrange function that rearranges the dimensions of the input tensor\n",
    "from einops.layers.torch import Rearrange # Rearrange Layer that rearranges the dimensions of the input tensor\n",
    "from torch import einsum # Einsum function that performs a contraction on the input tensor, similar to a Convolutional Layer\n",
    "from torchvision import transforms # Transforms that are used for data augmentation\n",
    "from torchsummary import summary # Summary function that displays the architecture of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "As we did in Notebook 1, we will have to download the dataset. This time the file is stored as a zip file, so we will need to extract it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file(url=\"https://www.dropbox.com/s/x70hm8mxqhe7fa6/bee_vs_wasp.tar.gz?dl=1\", filename=\"bee_vs_wasp.tar.gz\"):\n",
    "\n",
    "    # Download the file using requests\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Create a file object and write the response content in chunks\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    # Wait for the file to finish downloading\n",
    "    while not os.path.exists(filename):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Print a success message\n",
    "    print(f\"Downloaded {filename} successfully.\")\n",
    "\n",
    "def extract_file(filename, data_folder):\n",
    "    # Check if the file is a tar file\n",
    "    if tarfile.is_tarfile(filename):\n",
    "        # Open the tar file\n",
    "        tar = tarfile.open(filename, \"r:gz\")\n",
    "        # Extract all the files to the data folder\n",
    "        tar.extractall(data_folder)\n",
    "        # Close the tar file\n",
    "        tar.close()\n",
    "        # Print a success message\n",
    "        print(f\"Extracted {filename} to {data_folder} successfully.\")\n",
    "    else:\n",
    "        # Print an error message\n",
    "        print(f\"{filename} is not a valid tar file.\")\n",
    "    \n",
    "def manage_data(folder_name='bee_vs_wasp'):\n",
    "    '''Try to find the data for the exercise and return the path'''\n",
    "    \n",
    "    # Check common paths of where the data might be on different systems\n",
    "    likely_paths= [os.path.normpath(f'/blue/practicum-ai/share/data/{folder_name}'),\n",
    "                   os.path.normpath(f'/project/scinet_workshop2/data/{folder_name}'),\n",
    "                   os.path.join('data', folder_name),\n",
    "                   os.path.normpath(folder_name)]\n",
    "    \n",
    "    for path in likely_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f'Found data at {path}.')\n",
    "            return path\n",
    "\n",
    "    answer = input(f'Could not find data in the common locations. Do you know the path? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "        path = os.path.join(os.path.normpath(input('Please enter the path to the data folder: ')),folder_name)\n",
    "        if os.path.exists(path):\n",
    "            print(f'Thanks! Found your data at {path}.')\n",
    "            return path\n",
    "        else:\n",
    "            print(f'Sorry, that path does not exist.')\n",
    "    \n",
    "    answer = input('Do you want to download the data? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "\n",
    "        ''' Check and see if the downloaded data is inside the .gitignore file, and adds them to the list of files to ignore if not. \n",
    "        This is to prevent the data from being uploaded to the repository, as the files are too large for GitHub.'''\n",
    "        \n",
    "        if os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'r') as f:\n",
    "                ignore = f.read().split('\\n')\n",
    "        # If the .gitignore file does not exist, create a new one\n",
    "        elif not os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'w') as f:\n",
    "                f.write('')\n",
    "            ignore = []\n",
    "        else:\n",
    "            ignore = []\n",
    "\n",
    "        # Check if the .gz file is in the ignore list\n",
    "        if 'bee_vs_wasp.tar.gz' not in ignore:\n",
    "            ignore.append('bee_vs_wasp.tar.gz')\n",
    "            \n",
    "        # Check if the data/ folder is in the ignore list\n",
    "        if 'data/' not in ignore:\n",
    "            ignore.append('data/')\n",
    "\n",
    "        # Write the updated ignore list back to the .gitignore file\n",
    "        with open('.gitignore', 'w') as f:\n",
    "            f.write('\\n'.join(ignore))\n",
    "\n",
    "        print(\"Updated .gitignore file.\")\n",
    "        print('Downloading data, this may take a minute.')\n",
    "        download_file()\n",
    "        print('Data downloaded, unpacking')\n",
    "        extract_file(\"bee_vs_wasp.tar.gz\", \"data\")\n",
    "        print('Data downloaded and unpacked. Now available at data/bee_vs_wasp.')\n",
    "        return os.path.normpath('data/bee_vs_wasp')   \n",
    "\n",
    "    print('Sorry, I cannot find the data. Please download it manually from https://www.dropbox.com/s/x70hm8mxqhe7fa6/bee_vs_wasp.tar.gz and unpack it to the data folder.')      \n",
    "\n",
    "\n",
    "data_path = manage_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the dataset\n",
    "\n",
    "We will take a look at the dataset to see what it contains. We will also look at the annotations file, which contains the bounding box information for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_data(data_path):\n",
    "    '''Check the contents of the data folder'''\n",
    "    \n",
    "    # Check the contents of the data folder\n",
    "    data_folder = pathlib.Path(data_path)\n",
    "    # Check if the data folder exists\n",
    "    if data_folder.exists():\n",
    "        # Create a list of the contents of the data folder\n",
    "        data_folder_contents = list(data_folder.glob('*/*'))\n",
    "        # Create a list of the classes in the data folder\n",
    "        classes = [item.name for item in data_folder.glob('*') if item.is_dir()]\n",
    "        # Print the classes in the data folder\n",
    "        print(f'The data folder contains the following classes: {classes}')\n",
    "        # Create a dictionary to store the class distribution\n",
    "        class_distribution = {}\n",
    "        # Create a list to store the paths of the images\n",
    "        image_paths = []\n",
    "        # Create a list to store the labels of the images\n",
    "        labels = []\n",
    "        # Loop through the classes in the data folder\n",
    "        for class_name in classes:\n",
    "            # Create a list of the contents of the class folder\n",
    "            class_folder_contents = list(data_folder.glob(f'{class_name}/*'))\n",
    "            # Filter out any directories that start with \".ipynb_checkpoints\"\n",
    "            class_folder_contents = [item for item in class_folder_contents if not item.name.startswith('.ipynb_checkpoints')]\n",
    "            # Print the number of images in the class folder\n",
    "            print(f'The class {class_name} contains {len(class_folder_contents)} images.')\n",
    "            # Update the class distribution dictionary with the class name and the number of images\n",
    "            class_distribution[class_name] = len(class_folder_contents)\n",
    "            # Add the paths of the images to the image paths list\n",
    "            image_paths.extend(class_folder_contents[:4])\n",
    "            # Add the labels of the images to the labels list\n",
    "            labels.extend([class_name]*4)\n",
    "        # Create a figure to display the images\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(20, 10))\n",
    "        # Loop through the image paths and labels\n",
    "        for i, (image_path, label) in enumerate(zip(image_paths, labels)):\n",
    "            # Load the image\n",
    "            image = Image.open(image_path)\n",
    "            # Display the image\n",
    "            axes[i//4, i%4].imshow(image)\n",
    "            axes[i//4, i%4].set_title(label)\n",
    "            axes[i//4, i%4].axis('off')\n",
    "        # Show the figure\n",
    "        plt.show()\n",
    "        # Create a figure to display the class distribution\n",
    "        fig, ax = plt.subplots()\n",
    "        # Plot the class distribution as a histogram\n",
    "        ax.bar(class_distribution.keys(), class_distribution.values())\n",
    "        # Set the title of the plot\n",
    "        ax.set_title('Class Distribution')\n",
    "        # Set the x-label of the plot\n",
    "        ax.set_xlabel('Class')\n",
    "\n",
    "        # Set the y-label of the plot\n",
    "        ax.set_ylabel('Number of Images')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Print an error message if the data folder does not exist\n",
    "        print(f'The data folder {data_folder} does not exist.')\n",
    "\n",
    "check_data(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot to unpack here! First, depending on the images sampled, we can see that the images are of different sizes and have different numbers of fruits. Other things to note:\n",
    "- Some of the images don't really contain fruits...\n",
    "- The file names are the same as the image names, but with a .txt extension.\n",
    "- The annotations file contains the class ID of the fruit (0 corresponds to 'Apple', etc.), and the bounding box coordinates. \n",
    "- The bounding box coordinates are in the format (x_min, y_min, x_max, y_max).\n",
    "- The bounding box coordinates are normalized, meaning that they are scaled to be between 0 and 1. This is a common practice in object detection tasks.\n",
    "- The bounding boxes in some of the images are not very accurate. This is a common problem in object detection tasks.\n",
    "- The dataset is very imbalanced, with a lot more oranges than other fruits.\n",
    "\n",
    "## 4. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up our ViT model architecture\n",
    "\n",
    "# Define the Patch Embedding Layer\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Rearrange the input tensor x to have a new shape\n",
    "        x = self.projection(x)\n",
    "        x = rearrange(x, 'b e (h) (w) -> b (h w) e', h=x.shape[2], w=x.shape[3])\n",
    "        return x\n",
    "    \n",
    "# Define the Multi-Head Self-Attention Layer\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.fc_out = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the input tensor x into num_heads pieces\n",
    "        queries = rearrange(self.queries(x), 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        values = rearrange(self.values(x), 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        \n",
    "        # Perform a scaled dot-product attention\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) / (self.emb_size ** 0.5)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Apply the attention to the values\n",
    "        out = torch.einsum('bhal, bhlv -> bhav', attention, values)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "# Define the Multi-Layer Perceptron (MLP) Layer\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, emb_size, mlp_hidden_dim, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_size, mlp_hidden_dim)\n",
    "        self.fc2 = nn.Linear(mlp_hidden_dim, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_hidden_dim, dropout=0.0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = MLP(emb_size, mlp_hidden_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x)))\n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "# Define the Vision Transformer (ViT) Model\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size, num_classes, num_layers, num_heads, mlp_hidden_dim, dropout=0.0):\n",
    "        super(ViT, self).__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embedding = PatchEmbedding(in_channels, patch_size, emb_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_size))\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            Transformer(emb_size, num_heads, mlp_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(emb_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a ViT model instance\n",
    "\n",
    "# Define the model hyperparameters\n",
    "in_channels = 3\n",
    "patch_size = 16\n",
    "emb_size = 768\n",
    "img_size = 128\n",
    "num_classes = 6\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "mlp_hidden_dim = 3072\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "model = ViT(in_channels, patch_size, emb_size, img_size, num_classes, num_layers, num_heads, mlp_hidden_dim, dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the training function\n",
    "def train(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(data_loader)\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return running_loss / len(data_loader), correct / total\n",
    "\n",
    "# Define the test function\n",
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'train'), transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the validation dataset\n",
    "val_dataset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'valid'), transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=os.path.join(data_dir, 'test'), transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the results\n",
    "\n",
    "Let's look at those evaluation metrics we mentioned above. YOLOv8 creates a **runs** folder that stores each training run. We'll pull them up here and examine what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation results\n",
    "\n",
    "# Find the latest training run\n",
    "training = sorted(os.listdir('runs/detect/.'))\n",
    "latest_training = training[-1]\n",
    "print(f'Latest training run: {latest_training}')\n",
    "\n",
    "# Plot the .png files in the latest training run\n",
    "for file in os.listdir(f'runs/detect/{latest_training}'):\n",
    "    if file.endswith('.png'):\n",
    "        # Exclude the normalized confusion matrix since it's redundant\n",
    "        if 'normalized' in file:\n",
    "            continue\n",
    "        img = Image.open(f'runs/detect/{latest_training}/{file}')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the graphs\n",
    "If your stats are a little rusty like Kevin's, you might need a bit of refresher to make sense of the graphs above. Expand the section below for a rundown on the terms used above.\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary><h2>Click to Expand for Stats Terms!</h2></summary>\n",
    "\n",
    "<p>\n",
    "\n",
    "##### What is Precision?\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total predicted apples. Higher precision values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Recall?\n",
    "Recall is the ratio of correctly predicted positive observations to the all observations in actual class. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total actual apples. Higher recall values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Confidence?\n",
    "Confidence is the probability that a model assigns to a prediction. In our fruit object detection task, the confidence is the probability that a model assigns to a fruit being an apple, orange, or any other fruit. Higher confidence values are better, as they indicate that the model is more certain about its predictions.\n",
    "\n",
    "##### What is a confusion matrix?\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "##### What is the F1 Confidence Curve?\n",
    "The F1 Confidence Curve is a plot of the F1 score against the confidence threshold. The F1 score is the harmonic mean of precision and recall, and it is a measure of a model's accuracy. The confidence threshold is the minimum confidence level that a model must have in order to make a prediction. The F1 Confidence Curve is used to determine the optimal confidence threshold for a model. The curve shows how the F1 score changes as the confidence threshold is varied. The goal is to find the confidence threshold that maximizes the F1 score. The highest point on the curve tells you the optimal confidence threshold where the model strikes the best balance between precision and recall. In our fruit object detection task, the F1 Confidence Curve would show how the F1 score changes as the confidence threshold is varied for each fruit class. Higher F1 scores are better, as they indicate that the model is making more accurate predictions.\n",
    "    \n",
    "##### What is mAP50?\n",
    "mAP50 stands for \"mean Average Precision at 50% confidence\". It is a common metric used in object detection tasks to evaluate the performance of a model. The mAP50 is the average of the precision values at different recall levels, where the recall is calculated at a confidence threshold of 50%. A higher mAP50 value indicates better performance.\n",
    "    \n",
    "##### What is mAP50-95?\n",
    "mAP50-95 is the mean average precision (mAP) over the range of intersection over union (IoU) thresholds from 0.5 to 0.95. IoU is a measure of the overlap between two bounding boxes. It is calculated as the area of the intersection of the two bounding boxes divided by the area of their union. An IoU of 0 means that the two bounding boxes do not overlap at all, while an IoU of 1 means that the two bounding boxes are identical. The mAP50-95 metric is a more comprehensive metric than the mAP50 metric, which only considers the IoU threshold of 0.5. The mAP50-95 metric provides a more complete picture of the model's performance across a range of IoU thresholds.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "In addition to those graphs, we can look at the predictions themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the labels and predictions from the last training run\n",
    "\n",
    "# Load the images\n",
    "img1 = Image.open(f'runs/detect/{latest_training}/val_batch0_labels.jpg')\n",
    "img2 = Image.open(f'runs/detect/{latest_training}/val_batch0_pred.jpg')\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axes[0].imshow(img1)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Ground Truth')\n",
    "axes[1].imshow(img2)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the prediction images, you'll probably notice this model loves its citrus. That's no surprise given the histogram we looked at at the beginning of this notebook. What could we do to improve this?\n",
    "\n",
    "## 7. Inference\n",
    "How does the model fair on some test images? After you run the cell below:\n",
    "1. Find your own image of fruit.\n",
    "2. Upload it to this folder.\n",
    "3. Add or edit the code below to run on the new image rather than images in the test folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the model on a few test images\n",
    "data_dir = r\"datasets/fruits_detection\"\n",
    "\n",
    "# Get ten random test images\n",
    "test_images = []\n",
    "for folder in os.listdir(os.path.join(data_dir, 'test', 'images')):\n",
    "    img_path = os.path.join(data_dir, 'test', 'images', folder)\n",
    "    test_images.append(img_path)\n",
    "test_images = random.sample(test_images, 10)\n",
    "\n",
    "# Plot the test images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, img_path in enumerate(test_images):\n",
    "    img = Image.open(img_path)\n",
    "    axes[i // 5, i % 5].imshow(img)\n",
    "    axes[i // 5, i % 5].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Run the model on the test images\n",
    "infer_results = model.predict(test_images)\n",
    "\n",
    "for img_path, result in zip(test_images, infer_results):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((640, 640))\n",
    "\n",
    "    # Create the plot for this image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Access bounding boxes and class information\n",
    "    boxes = result.boxes\n",
    "    class_ids = boxes.cls.cpu().numpy()  # Class IDs are on the CPU\n",
    "    confidences = boxes.conf.cpu().numpy()  # Confidences are on the CPU\n",
    "\n",
    "    for box, class_id, conf in zip(boxes.xyxy, class_ids, confidences):  \n",
    "        # boxes.xyxy are in [xmin, ymin, xmax, ymax] format\n",
    "        x1, y1, x2, y2 = box.cpu()  # Move coordinates to CPU\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        rect = plt.Rectangle((x1, y1), w, h, fill=False, color='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.text(x1, y1, result.names[int(class_id)], color='red')  \n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explore hyperparameters\n",
    "\n",
    "Now that you have a good baseline, consider how you might deal with this model's issues.\n",
    "- How would you address issues in the dataset?\n",
    "- How would you optimize training?\n",
    "\n",
    "A good first place to start would be YOLOv8's documentation so you can understand what hyperparameters you have access to and how changing them will affect training. Make some adjustments and see how high you can get your fruits object detection F1 score!\n",
    "\n",
    "## Bonus Exercises\n",
    "\n",
    "- You might have noticed the *yolov8n.pt* file that is added to the folder when you load YOLO. That is the pre-trained model using the [COCO dataset](https://cocodataset.org/#home). The YOLOv8 documentation linked above provides instructions for transer learning and fine-tuning with the pre-trained model. Give it a shot!\n",
    "\n",
    "- If you're feeling very bold and want the full Object Detection Experience, find an image labeler and build your own dataset! If you use proprietary data, be sure that the software you use to label meets your institution's guidelines for data sharing. If you go this route, you will almost certainly want to use the fine-tuning option mentioned above, otherwise you're going to be spending quite a long time labeling..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
