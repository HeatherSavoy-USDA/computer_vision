{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Bonus Notebook\n",
    "\n",
    "This notebook is a bonus notebook that demonstrates how to use the Vision Transformer (ViT) model for image classification. The dataset used is the [Bees vs Wasps dataset](https://www.kaggle.com/datasets/jerzydziewierz/bee-vs-wasp) from Kaggle. The dataset contains images of fruits and their bounding boxes. The goal, as in notebooks 1 and 1.2 is to classify the images into one of the four classes.\n",
    "\n",
    "As before, the dataset was found on Kaggle. [Check out the dataset information](https://www.kaggle.com/datasets/jerzydziewierz/bee-vs-wasp)\n",
    "\n",
    "<img src=\"notebook_images/vit_cover_image.png\" \n",
    "        alt=\"Image of an insect broken into transformer tokens\" \n",
    "        width=\"1000\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "In this notebook we will code our own ViT model from scratch using PyTorch. We will also use the `torchvision` library to load the dataset and preprocess the images. Vision Transformers (ViTs) are a newer class of models that have shown great promise in image classification tasks. They are based on the Transformer architecture, which was originally designed for natural language processing tasks. ViTs have shown to be competitive with CNNs on image classification tasks, and have the added advantage of being able to capture long-range dependencies in the data. A downside of ViTs is that they can be computationally expensive to train, and require larger amounts of data to perform well versus CNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing supporting libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "import tarfile\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Importing the necessary libraries for the dataset\n",
    "import numpy as np # NumPy\n",
    "from PIL import Image # Python Imaging Library\n",
    "import pandas as pd # Pandas\n",
    "from sklearn.model_selection import train_test_split # Train-Test Split\n",
    "from sklearn.preprocessing import StandardScaler # Standard Scaler\n",
    "from sklearn.datasets import fetch_openml # Fetch OpenML\n",
    "from sklearn.metrics import accuracy_score # Accuracy Score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Importing the necessary libraries for the visualization\n",
    "import matplotlib.pyplot as plt # Matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Importing the necessary libraries for ViT\n",
    "import torch # PyTorch\n",
    "import torch.nn as nn # Neural Network Module\n",
    "import torch.nn.functional as F # Functional Module that contains activation functions\n",
    "import torchvision # TorchVision\n",
    "from tqdm import tqdm # TQDM for displaying progress bar\n",
    "from einops import rearrange # Rearrange function that rearranges the dimensions of the input tensor\n",
    "from einops.layers.torch import Rearrange # Rearrange Layer that rearranges the dimensions of the input tensor\n",
    "from torch import einsum # Einsum function that performs a contraction on the input tensor, similar to a Convolutional Layer\n",
    "from torchvision import transforms # Transforms that are used for data augmentation\n",
    "# from torchsummary import summary # Summary function that displays the architecture of the model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "As we did in Notebook 1, we will have to download the dataset. This time the file is stored as a zip file, so we will need to extract it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file(url=\"https://www.dropbox.com/s/x70hm8mxqhe7fa6/bee_vs_wasp.tar.gz?dl=1\", filename=\"bee_vs_wasp.tar.gz\"):\n",
    "\n",
    "    # Download the file using requests\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Create a file object and write the response content in chunks\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    # Wait for the file to finish downloading\n",
    "    while not os.path.exists(filename):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Print a success message\n",
    "    print(f\"Downloaded {filename} successfully.\")\n",
    "\n",
    "def extract_file(filename, data_folder):\n",
    "    # Check if the file is a tar file\n",
    "    if tarfile.is_tarfile(filename):\n",
    "        # Open the tar file\n",
    "        tar = tarfile.open(filename, \"r:gz\")\n",
    "        # Extract all the files to the data folder\n",
    "        tar.extractall(data_folder)\n",
    "        # Close the tar file\n",
    "        tar.close()\n",
    "        # Print a success message\n",
    "        print(f\"Extracted {filename} to {data_folder} successfully.\")\n",
    "    else:\n",
    "        # Print an error message\n",
    "        print(f\"{filename} is not a valid tar file.\")\n",
    "    \n",
    "def manage_data(folder_name='bee_vs_wasp'):\n",
    "    '''Try to find the data for the exercise and return the path'''\n",
    "    \n",
    "    # Check common paths of where the data might be on different systems\n",
    "    likely_paths= [os.path.normpath(f'/blue/practicum-ai/share/data/{folder_name}'),\n",
    "                   os.path.normpath(f'/project/scinet_workshop2/data/{folder_name}'),\n",
    "                   os.path.join('data', folder_name),\n",
    "                   os.path.normpath(folder_name)]\n",
    "    \n",
    "    for path in likely_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f'Found data at {path}.')\n",
    "            return path\n",
    "\n",
    "    answer = input(f'Could not find data in the common locations. Do you know the path? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "        path = os.path.join(os.path.normpath(input('Please enter the path to the data folder: ')),folder_name)\n",
    "        if os.path.exists(path):\n",
    "            print(f'Thanks! Found your data at {path}.')\n",
    "            return path\n",
    "        else:\n",
    "            print(f'Sorry, that path does not exist.')\n",
    "    \n",
    "    answer = input('Do you want to download the data? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "\n",
    "        ''' Check and see if the downloaded data is inside the .gitignore file, and adds them to the list of files to ignore if not. \n",
    "        This is to prevent the data from being uploaded to the repository, as the files are too large for GitHub.'''\n",
    "        \n",
    "        if os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'r') as f:\n",
    "                ignore = f.read().split('\\n')\n",
    "        # If the .gitignore file does not exist, create a new one\n",
    "        elif not os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'w') as f:\n",
    "                f.write('')\n",
    "            ignore = []\n",
    "        else:\n",
    "            ignore = []\n",
    "\n",
    "        # Check if the .gz file is in the ignore list\n",
    "        if 'bee_vs_wasp.tar.gz' not in ignore:\n",
    "            ignore.append('bee_vs_wasp.tar.gz')\n",
    "            \n",
    "        # Check if the data/ folder is in the ignore list\n",
    "        if 'data/' not in ignore:\n",
    "            ignore.append('data/')\n",
    "\n",
    "        # Write the updated ignore list back to the .gitignore file\n",
    "        with open('.gitignore', 'w') as f:\n",
    "            f.write('\\n'.join(ignore))\n",
    "\n",
    "        print(\"Updated .gitignore file.\")\n",
    "        print('Downloading data, this may take a minute.')\n",
    "        download_file()\n",
    "        print('Data downloaded, unpacking')\n",
    "        extract_file(\"bee_vs_wasp.tar.gz\", \"data\")\n",
    "        print('Data downloaded and unpacked. Now available at data/bee_vs_wasp.')\n",
    "        return os.path.normpath('data/bee_vs_wasp')   \n",
    "\n",
    "    print('Sorry, I cannot find the data. Please download it manually from https://www.dropbox.com/s/x70hm8mxqhe7fa6/bee_vs_wasp.tar.gz and unpack it to the data folder.')      \n",
    "\n",
    "\n",
    "data_path = manage_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the dataset\n",
    "\n",
    "We will take a look at the dataset to see what it contains.\n",
    "\n",
    "!!! Make Notes Here !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_display_data(data_path):\n",
    "    '''Check the contents of the data folder'''\n",
    "    \n",
    "    # Check the contents of the data folder\n",
    "    data_folder = pathlib.Path(data_path)\n",
    "    # Check if the data folder exists\n",
    "    if data_folder.exists():\n",
    "        # Create a list of the contents of the data folder\n",
    "        data_folder_contents = list(data_folder.glob('*/*'))\n",
    "        # Create a list of the classes in the data folder\n",
    "        classes = [item.name for item in data_folder.glob('*') if item.is_dir()]\n",
    "        # Print the classes in the data folder\n",
    "        print(f'The data folder contains the following classes: {classes}')\n",
    "        # Create a dictionary to store the class distribution\n",
    "        class_distribution = {}\n",
    "        # Create a list to store the paths of the images\n",
    "        image_paths = []\n",
    "        # Create a list to store the labels of the images\n",
    "        labels = []\n",
    "        # Loop through the classes in the data folder\n",
    "        for class_name in classes:\n",
    "            # Create a list of the contents of the class folder\n",
    "            class_folder_contents = list(data_folder.glob(f'{class_name}/*'))\n",
    "            # Filter out any directories that start with \".ipynb_checkpoints\"\n",
    "            class_folder_contents = [item for item in class_folder_contents if not item.name.startswith('.ipynb_checkpoints')]\n",
    "            # Print the number of images in the class folder\n",
    "            print(f'The class {class_name} contains {len(class_folder_contents)} images.')\n",
    "            # Update the class distribution dictionary with the class name and the number of images\n",
    "            class_distribution[class_name] = len(class_folder_contents)\n",
    "            # Add the paths of the images to the image paths list\n",
    "            image_paths.extend(class_folder_contents[:4])\n",
    "            # Add the labels of the images to the labels list\n",
    "            labels.extend([class_name]*4)\n",
    "        # Create a figure to display the images\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(20, 10))\n",
    "        # Loop through the image paths and labels\n",
    "        for i, (image_path, label) in enumerate(zip(image_paths, labels)):\n",
    "            # Load the image\n",
    "            image = Image.open(image_path)\n",
    "            # Display the image\n",
    "            axes[i//4, i%4].imshow(image)\n",
    "            axes[i//4, i%4].set_title(label)\n",
    "            axes[i//4, i%4].axis('off')\n",
    "        # Show the figure\n",
    "        plt.show()\n",
    "\n",
    "        # Create train and validation dataframes\n",
    "        train_data = []\n",
    "        for class_name in classes:\n",
    "            class_folder_contents = list(data_folder.glob(f'{class_name}/*'))\n",
    "            class_folder_contents = [item for item in class_folder_contents if not item.name.startswith('.ipynb_checkpoints')]\n",
    "            for item in class_folder_contents:\n",
    "                train_data.append((item, class_name))\n",
    "        train_df = pd.DataFrame(train_data, columns=['image', 'class'])\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['class'], random_state=42)\n",
    "        print(f'The training dataset contains {len(train_df)} images.')\n",
    "        print(f'The validation dataset contains {len(val_df)} images.')\n",
    "\n",
    "        # Create a figure to display the class distribution ordered from highest to lowest\n",
    "        fig, ax = plt.subplots() \n",
    "        # Sort the class distribution dictionary by the number of images\n",
    "        class_distribution = dict(sorted(class_distribution.items(), key=lambda item: item[1], reverse=True))\n",
    "        # Plot the class distribution as a histogram\n",
    "        ax.bar(class_distribution.keys(), class_distribution.values())\n",
    "        # Set the title of the plot\n",
    "        ax.set_title('Class Distribution')\n",
    "        # Set the x-label of the plot\n",
    "        ax.set_xlabel('Class')\n",
    "\n",
    "        # Set the y-label of the plot\n",
    "        ax.set_ylabel('Number of Images')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Print an error message if the data folder does not exist\n",
    "        print(f'The data folder {data_folder} does not exist.')\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = prep_display_data(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Make Notes Here !!!\n",
    "\n",
    "## 4. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up our ViT model architecture\n",
    "\n",
    "# Define the Patch Embedding Layer\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Rearrange the input tensor x to have a new shape\n",
    "        x = self.projection(x)\n",
    "        x = rearrange(x, 'b e (h) (w) -> b (h w) e', h=x.shape[2], w=x.shape[3])\n",
    "        return x\n",
    "    \n",
    "# Define the Multi-Head Self-Attention Layer\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout=0.0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        self.keys = nn.Linear(emb_size, emb_size)\n",
    "        self.queries = nn.Linear(emb_size, emb_size)\n",
    "        self.values = nn.Linear(emb_size, emb_size)\n",
    "        self.fc_out = nn.Linear(emb_size, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the input tensor x into num_heads pieces\n",
    "        queries = rearrange(self.queries(x), 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        keys = rearrange(self.keys(x), 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        values = rearrange(self.values(x), 'b n (h d) -> b h n d', h=self.num_heads)\n",
    "        \n",
    "        # Perform a scaled dot-product attention\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) / (self.emb_size ** 0.5)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Apply the attention to the values\n",
    "        out = torch.einsum('bhal, bhlv -> bhav', attention, values)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "# Define the Multi-Layer Perceptron (MLP) Layer\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, emb_size, mlp_hidden_dim, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_size, mlp_hidden_dim)\n",
    "        self.fc2 = nn.Linear(mlp_hidden_dim, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_hidden_dim, dropout=0.0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = MLP(emb_size, mlp_hidden_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x)))\n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "# Define the Vision Transformer (ViT) Model\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size, num_classes, num_layers, num_heads, mlp_hidden_dim, dropout=0.0):\n",
    "        super(ViT, self).__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_embedding = PatchEmbedding(in_channels, patch_size, emb_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_size))\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            Transformer(emb_size, num_heads, mlp_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(emb_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a ViT model instance\n",
    "\n",
    "# Define the model hyperparameters\n",
    "in_channels = 3\n",
    "patch_size = 16\n",
    "emb_size = 768\n",
    "img_size = 128\n",
    "num_classes = 6\n",
    "num_layers = 12\n",
    "num_heads = 12\n",
    "mlp_hidden_dim = 3072\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "model = ViT(in_channels, patch_size, emb_size, img_size, num_classes, num_layers, num_heads, mlp_hidden_dim, dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 What is a Vision Transformer \"seeing\"?\n",
    "\n",
    "!!! Make Notes Here !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show a random image from the dataset\n",
    "image_path = random.choice(train_df['image'])\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Show the image broken up into patches\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "patch_size = 16\n",
    "patches = rearrange(image_tensor, 'b c (h s1) (w s2) -> b (h w) c s1 s2', s1=patch_size, s2=patch_size) # Rearrange the dimensions of the input tensor\n",
    "n_patches = patches.shape[1] # Get the number of patches\n",
    "n_patches_side = int(np.sqrt(n_patches))\n",
    "fig, axes = plt.subplots(n_patches_side, n_patches_side, figsize=(10, 10))\n",
    "\n",
    "for i in range(n_patches):\n",
    "    patch = patches[0, i].permute(1, 2, 0)\n",
    "    axes[i // n_patches_side, i % n_patches_side].imshow(patch)\n",
    "    axes[i // n_patches_side, i % n_patches_side].axis('off')\n",
    "\n",
    "# Show the image patches\n",
    "plt.imshow(patch)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "patch = patches[0, i].unsqueeze(0).permute(0, 2, 3, 1).squeeze(0)\n",
    "patches = patches[:, torch.randperm(patches.size(1))]\n",
    "\n",
    "# Show the image patches plotted randomly in a single row\n",
    "fig, ax = plt.subplots(1, n_patches, figsize=(20, 20))\n",
    "for i in range(n_patches):\n",
    "    patch = patches[0, i].permute(1, 2, 0)\n",
    "    ax[i].imshow(patch)\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Make Notes Here !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Fit the model to the data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Define the training function\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, data_loader, criterion, optimizer, device):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Fit the model to the data\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Define the training function\n",
    "def train(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    total_batches = len(data_loader)\n",
    "    train_progress_bar = tqdm(range(total_batches), desc=\"Training\", leave=False)\n",
    "    for batch_num, (images, labels) in enumerate(data_loader):\n",
    "        train_progress_bar.set_description(f\"Training Batch {batch_num+1}/{total_batches}\")\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_progress_bar.update()\n",
    "    train_progress_bar.close()\n",
    "    return running_loss / len(data_loader), train_correct / train_total\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_progress_bar = tqdm(data_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            val_progress_bar.set_description(\"Validating\")\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "            val_progress_bar.update()\n",
    "    val_progress_bar.close()\n",
    "    return running_loss / len(data_loader), val_correct / val_total\n",
    "\n",
    "# Define the test function\n",
    "'''Currently, the test function is not being used in the training loop. \n",
    "However, you can use it to evaluate the model on the test dataset after training.'''\n",
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_progress_bar = tqdm(data_loader, desc=\"Testing\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            test_progress_bar.set_description(\"Testing\")\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_progress_bar.update()\n",
    "    test_progress_bar.close()\n",
    "    return correct / total\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the training dataframe\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = torchvision.datasets.ImageFolder(data_path, transform=transform)\n",
    "\n",
    "# Create the training data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load the validation dataframe\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = torchvision.datasets.ImageFolder(data_path, transform=transform)\n",
    "\n",
    "# Create the validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracies)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracies: .4f} Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the results\n",
    "\n",
    "!!! Make Notes Here !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix, without Normalization'\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True Label',\n",
    "           xlabel='Predicted Label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "# Get the predictions for the validation dataset\n",
    "y_pred = []\n",
    "y_true = []\n",
    "val_progress_bar = tqdm(val_loader, desc=\"Predicting\", leave=False)\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "val_progress_bar.close()\n",
    "\n",
    "# Pull the class names from the dataset\n",
    "classes = val_dataset.classes\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! Add Notes Here !!!\n",
    "\n",
    "## 7. Inference\n",
    "How does the model fair on some test images? Lets try some new images!\n",
    "1. Find your own image image.\n",
    "2. Upload it to this folder.\n",
    "3. Add or edit the code below to run on the new image rather than images in the test folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the model on a few test imagesdata_dir = r\"datasets/fruits_detection\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Get ten random test images\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_images \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m folder \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdata_dir\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m      6\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m, folder)\n\u001b[0;32m      7\u001b[0m     test_images\u001b[38;5;241m.\u001b[39mappend(img_path)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the model on a few test images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explore hyperparameters\n",
    "\n",
    "Now that you have a good baseline, consider how you might deal with this model's issues.\n",
    "- How would you address issues in the dataset?\n",
    "- How would you optimize training?\n",
    "\n",
    "How does this model compare to the CNNs you've used? Training time? Accuracy? What are some of the hyperparameters that are in ViT that aren't in CNNs?\n",
    "\n",
    "!!! Insert Notes Here!!!\n",
    "\n",
    "## Bonus Exercises\n",
    "\n",
    "- !!! Insert Cool Ideas Here !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
