{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset from: https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/data, the dataset is Public Domain\n",
    "\n",
    "Some code is from https://www.kaggle.com/code/harpdeci/yolo-nas-fruit-detection/notebook\n",
    "\n",
    "# [Fruit Detection](https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook will be used for an Object Detection task that trains a model on the fruits_detection dataset using YOLOv8\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml \n",
    "from ultralytics import YOLO\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "'''from super_gradients.training import Trainer, dataloaders, models\n",
    "from super_gradients.training.dataloaders.dataloaders import (\n",
    "    coco_detection_yolo_format_train, coco_detection_yolo_format_val\n",
    ")\n",
    "from super_gradients.training.losses import PPYoloELoss\n",
    "from super_gradients.training.metrics import DetectionMetrics_050\n",
    "from super_gradients.training.models.detection_models.pp_yolo_e import (\n",
    "    PPYoloEPostPredictionCallback\n",
    ")'''\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url=\"https://www.dropbox.com/scl/fi/79fa4j2sjb0ohvlv843lr/fruits_detection.zip?rlkey=0sskhcwckcgvvwknuu6w21n5q&st=jx3vrsl8&dl=1\", filename=\"fruits_detection.zip\"):\n",
    "                        \n",
    "    # Check to see if the datasets folder exists\n",
    "    if not os.path.exists(\"datasets\"):\n",
    "        os.makedirs(\"datasets\")\n",
    "    \n",
    "    # Download the file using requests\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Create a file object and write the response content in chunks\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    # Wait for the file to finish downloading\n",
    "    while not os.path.exists(filename):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Print a success message\n",
    "    print(f\"Downloaded {filename} successfully.\")\n",
    "\n",
    "def extract_file(filename, data_folder):\n",
    "    # Check if the file is a zip file\n",
    "    if zipfile.is_zipfile(filename):\n",
    "        # Open the zip file\n",
    "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "            # Extract all the files to the data folder\n",
    "            zip_ref.extractall(data_folder)\n",
    "            # Print a success message\n",
    "            print(f\"Extracted {filename} to {data_folder} successfully.\")\n",
    "    else:\n",
    "        # Print an error message\n",
    "        print(type(filename))\n",
    "        print(f\"{filename} is not a valid zip file.\")\n",
    "    \n",
    "def manage_data(folder_name='fruits_detection'):\n",
    "    '''Try to find the data for the exercise and return the path'''\n",
    "    \n",
    "    # Check common paths of where the data might be on different systems\n",
    "    likely_paths= [os.path.normpath(f'/blue/practicum-ai/share/data/{folder_name}'),\n",
    "                   os.path.normpath(f'/project/scinet_workshop2/data/{folder_name}'),\n",
    "                   os.path.join('datasets', folder_name),\n",
    "                   os.path.normpath(folder_name)]\n",
    "    \n",
    "    for path in likely_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f'Found data at {path}.')\n",
    "            return path\n",
    "\n",
    "    answer = input(f'Could not find data in the common locations. Do you know the path? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "        path = os.path.join(os.path.normpath(input('Please enter the path to the data folder: ')),folder_name)\n",
    "        if os.path.exists(path):\n",
    "            print(f'Thanks! Found your data at {path}.')\n",
    "            return path\n",
    "        else:\n",
    "            print(f'Sorry, that path does not exist.')\n",
    "    \n",
    "    answer = input('Do you want to download the data? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "\n",
    "        ''' Check and see if the downloaded data is inside the .gitignore file, and adds them to the list of files to ignore if not. \n",
    "        This is to prevent the data from being uploaded to the repository, as the files are too large for GitHub.'''\n",
    "        \n",
    "        if os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'r') as f:\n",
    "                ignore = f.read().split('\\n')\n",
    "        # If the .gitignore file does not exist, create a new one\n",
    "        elif not os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'w') as f:\n",
    "                f.write('')\n",
    "            ignore = []\n",
    "        else:\n",
    "            ignore = []\n",
    "\n",
    "        # Check if the .gz file is in the ignore list\n",
    "        if 'fruits_detection.zip' not in ignore:\n",
    "            ignore.append('fruits_detection.zip')\n",
    "            \n",
    "        # Check if the data/ folder is in the ignore list\n",
    "        if 'datasets/' not in ignore:\n",
    "            ignore.append('datasets/')\n",
    "\n",
    "        # Write the updated ignore list back to the .gitignore file\n",
    "        with open('.gitignore', 'w') as f:\n",
    "            f.write('\\n'.join(ignore))\n",
    "\n",
    "        print(\"Updated .gitignore file.\")\n",
    "        print('Downloading data, this may take a minute.')\n",
    "        download_file()\n",
    "        print('Data downloaded, unpacking')\n",
    "        extract_file(\"fruits_detection.zip\", \"datasets\")\n",
    "        print('Data downloaded and unpacked. Now available at datasets/fruits_detection.')\n",
    "        return os.path.normpath('datasets/fruits_detection')   \n",
    "\n",
    "    print('Sorry, I cannot find the data. Please download it manually from https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/ and unpack it to the datasets folder.')      \n",
    "\n",
    "\n",
    "data_path = manage_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a file from Kaggle using the Kaggle API into the datasets folder\n",
    "\n",
    "# Check if the Kaggle API key is present\n",
    "if os.path.exists('kaggle.json'):\n",
    "    # Load the Kaggle API key\n",
    "    with open('kaggle.json', 'r') as f:\n",
    "        kaggle = json.load(f)\n",
    "    # Save the Kaggle API key to the environment\n",
    "    os.environ['KAGGLE_USERNAME'] = kaggle['username']\n",
    "    os.environ['KAGGLE_KEY'] = kaggle['key']\n",
    "    # Download the dataset\n",
    "    os.system('kaggle datasets download lakshaytyagi01/fruit-detection -p datasets')\n",
    "else:\n",
    "    print('Please upload your Kaggle API key to this notebook.')\n",
    "\n",
    "# Unzip the downloaded file\n",
    "with zipfile.ZipFile('datasets/fruit-detection.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('datasets')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the path to the dataset\n",
    "data_dir = r\"data/fruits_detection\"\n",
    "\n",
    "# Make a histogram of the number of images in each class\n",
    "def explore_data(data_dir, show_picture=True, show_hist=True):\n",
    "    \n",
    "    # Examine some sample images\n",
    "    if show_picture:\n",
    "        # Get valid image folders \n",
    "        image_folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))] \n",
    "\n",
    "        sample_images = []\n",
    "        for i in range(5):\n",
    "            folder = random.choice(image_folders) \n",
    "            img_path = os.path.join(data_dir, folder, 'images', random.choice(os.listdir(os.path.join(data_dir, folder, 'images'))))\n",
    "            sample_images.append(img_path)\n",
    "\n",
    "        # Plot the sample images\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, img_path in enumerate(sample_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Make a histogram of the number of images in each class\n",
    "    if show_hist:\n",
    "        def get_class_counts(folder_path):  # Change from data_dir to folder_path\n",
    "            class_counts = {}\n",
    "            labels_path = os.path.join(folder_path, 'labels')  # Add labels path\n",
    "            for filename in os.listdir(labels_path):  # Update listdir\n",
    "                with open(os.path.join(labels_path, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        class_id = int(line.split(' ')[0])  # Assuming labels are in YOLO format\n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "            return class_counts\n",
    "\n",
    "        train_counts = get_class_counts(os.path.join(data_dir, 'train'))  # Add os.path.join\n",
    "        val_counts = get_class_counts(os.path.join(data_dir, 'valid'))\n",
    "        test_counts = get_class_counts(os.path.join(data_dir, 'test'))\n",
    "        class_names = ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']\n",
    "        num_classes = len(class_names)\n",
    "\n",
    "        data_counts = {\n",
    "            'train': pd.Series(train_counts),\n",
    "            'val': pd.Series(val_counts),\n",
    "            'test': pd.Series(test_counts)\n",
    "        }\n",
    "        df = pd.DataFrame(data_counts)\n",
    "\n",
    "        df.plot.bar(figsize=(10, 6))\n",
    "        plt.xlabel('Class Name')\n",
    "        plt.xticks(np.arange(num_classes), class_names)\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.title('Distribution of Images per Class')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "explore_data(data_dir, show_picture=True, show_hist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a YAML file for the YOLOv8 model configuration\n",
    "\n",
    "def create_yaml(data_dir, class_names, yaml_file='fruits_detection_data.yaml'):\n",
    "    \"\"\"\n",
    "    Creates a YOLOv8 data.yaml file. YAML stands for \"YAML Ain't Markup Language\" and is a human-readable data serialization format.\n",
    "    A YAML file is used to define the dataset configuration for training a YOLOv8 model.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Path to the dataset root directory.\n",
    "        class_names (list): List of class names.\n",
    "        yaml_file (str): Name of the YAML file to save. Defaults to 'data.yaml'.\n",
    "    \"\"\"\n",
    "\n",
    "    yaml_dict = {\n",
    "        # 'path': data_dir,  # Path to your dataset\n",
    "        'train': data_dir + '/train/images',  # Relative path to training images\n",
    "        'val': data_dir + '/valid/images',    # Relative path to validation images\n",
    "        'test': data_dir + '/test/images',    # Relative path to testing images\n",
    "\n",
    "        'num_classes': len(class_names),   # Number of classes\n",
    "        'names': class_names      # List of class names\n",
    "    }\n",
    "\n",
    "    with open(yaml_file, 'w') as outfile:\n",
    "        yaml.dump(yaml_dict, outfile, default_flow_style=False)\n",
    "\n",
    "    print(f'YAML file created: {yaml_file}')\n",
    "\n",
    "data_dir = 'data/fruits_detection'\n",
    "class_names = ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']\n",
    "\n",
    "create_yaml(data_dir, class_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the YOLOv8 model\n",
    "model = YOLO('yolov8n.yaml')\n",
    "results = model.train(data='fruits_detection_data.yaml', imgsz=640, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
