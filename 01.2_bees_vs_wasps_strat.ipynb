{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Concepts Continued\n",
    "\n",
    "Kevin now has a decent understanding of CNNs and how they work, but his model is still not performing as well as he would like. He's been reading up on some techniques to improve his model's performance and is excited to try them out. In this notebook, Kevin will learn about how to apply some of the class balancing techniques he's been learning from Amelia.\n",
    "\n",
    "In this notebook, Kevin will learn about stratified sampling and how to apply it to his dataset.\n",
    "\n",
    "## [!!!INSERT IMAGE ABOUT CLASS IMBALANCE!!!]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'lib' has no attribute 'X509_V_FLAG_NOTIFY_POLICY'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m   \u001b[38;5;66;03m# Import the TensorFlow library, which provides tools for deep learning.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m  \u001b[38;5;66;03m# Import the pandas library, used for data manipulation and analysis.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\__init__.py:37\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Compatibility functions.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mThe `tf.compat` module contains two sets of compatibility functions.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\__init__.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v1\\compat\\v2\\__init__.py:28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\__init__.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bitwise\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v2\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatibility_horizon\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forward_compatible\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorflow\\_api\\v2\\compat\\v2\\compat\\v2\\__init__.py:328\u001b[0m\n\u001b[0;32m    326\u001b[0m _current_module \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[0;32m    329\u001b[0m   _current_module\u001b[38;5;241m.\u001b[39m__path__ \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    330\u001b[0m       [_module_util\u001b[38;5;241m.\u001b[39mget_parent_dir(summary)] \u001b[38;5;241m+\u001b[39m _current_module\u001b[38;5;241m.\u001b[39m__path__)\n\u001b[0;32m    331\u001b[0m   \u001b[38;5;28msetattr\u001b[39m(_current_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, summary)\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\summary\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# If the V1 summary API is accessible, load and re-export it here.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m v1  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\summary\\v1.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _audio_summary\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom_scalar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _custom_scalar_summary\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _histogram_summary\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _image_summary\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpr_curve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary \u001b[38;5;28;01mas\u001b[39;00m _pr_curve_summary\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary.py:35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary_v2\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Export V3 versions.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m histogram \u001b[38;5;241m=\u001b[39m summary_v2\u001b[38;5;241m.\u001b[39mhistogram\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\plugins\\histogram\\summary_v2.py:35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhistogram\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metadata\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lazy_tensor_creator\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_util\n\u001b[0;32m     38\u001b[0m DEFAULT_BUCKET_COUNT \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhistogram_pb\u001b[39m(tag, data, buckets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\util\\tensor_util.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_pb2\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorflow_stub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes, compat, tensor_shape\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mExtractBitsFromFloat16\u001b[39m(x):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint16)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_graph_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m as_dtype  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DType  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m string  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Library of dtypes (Tensor element types).\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types_pb2\n\u001b[0;32m     22\u001b[0m _np_bfloat16 \u001b[38;5;241m=\u001b[39m pywrap_tensorflow\u001b[38;5;241m.\u001b[39mTF_bfloat16_type()\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\pywrap_tensorflow.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstruct\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m errors\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n\u001b[0;32m     25\u001b[0m TFE_DEVICE_PLACEMENT_WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     26\u001b[0m TFE_DEVICE_PLACEMENT_SILENT_FOR_INT32 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gfile\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\io\\gfile.py:33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     S3_ENABLED \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\boto3\\__init__.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _warn_deprecated_python\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Session\n\u001b[0;32m     19\u001b[0m __author__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmazon Web Services\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     20\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.34.89\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\boto3\\session.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataNotFoundError, UnknownServiceError\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\session.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msocket\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfigloader\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcredentials\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\client.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m waiter, xform_name\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01margs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientArgsCreator\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauth\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTH_TYPE_MAPS\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\waiter.py:18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjmespath\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaiterDocstring\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_service_module_name\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m xform_name\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\docs\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ServiceDocumenter\n\u001b[0;32m     17\u001b[0m DEPRECATED_SERVICE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msms-voice\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_docs\u001b[39m(root_dir, session):\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\docs\\service.py:14\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbcdoc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrestdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentStructure\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     ClientContextParamsDocumenter,\n\u001b[0;32m     16\u001b[0m     ClientDocumenter,\n\u001b[0;32m     17\u001b[0m     ClientExceptionsDocumenter,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpaginator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PaginatorDocumenter\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwaiter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WaiterDocumenter\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\docs\\client.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbcdoc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrestdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocumentStructure\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResponseExampleDocumenter\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     document_custom_method,\n\u001b[0;32m     21\u001b[0m     document_model_driven_method,\n\u001b[0;32m     22\u001b[0m     get_instance_public_methods,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparams\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResponseParamsDocumenter\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\docs\\example.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ANY KIND, either express or implied. See the License for the specific\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# language governing permissions and limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeDocumenter\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m py_default\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseExampleDocumenter\u001b[39;00m(ShapeDocumenter):\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\docs\\shape.py:19\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2015 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# inherited from a Documenter class with the appropriate methods\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# and attributes.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_json_value_header\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mShapeDocumenter\u001b[39;00m:\n\u001b[0;32m     23\u001b[0m     EVENT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\utils.py:39\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mawsrequest\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhttpsession\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# IP Regexes retained for backwards compatibility\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotocore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HEX_PAT  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\botocore\\httpsession.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# Always import the original SSLContext, even if it has been patched\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyopenssl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     46\u001b[0m             orig_util_SSLContext \u001b[38;5;28;01mas\u001b[39;00m SSLContext,\n\u001b[0;32m     47\u001b[0m         )\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mssl_\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSLContext\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py:50\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mTLS with SNI_-support for Python 2. Follow these instructions if you would\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mlike to verify TLS certificates in Python 2. Note, the default libraries do\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m.. _idna: https://github.com/kjd/idna\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrypto\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSSL\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcryptography\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m x509\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\OpenSSL\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (C) AB Strakt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# See LICENSE for details.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mpyOpenSSL - A simple wrapper around the OpenSSL library\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SSL, crypto\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     __author__,\n\u001b[0;32m     11\u001b[0m     __copyright__,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     __version__,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     21\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrypto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     32\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\OpenSSL\\SSL.py:19\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakValueDictionary\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     UNSPECIFIED \u001b[38;5;28;01mas\u001b[39;00m _UNSPECIFIED,\n\u001b[0;32m     11\u001b[0m     exception_from_error_queue \u001b[38;5;28;01mas\u001b[39;00m _exception_from_error_queue,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     text_to_bytes_and_warn \u001b[38;5;28;01mas\u001b[39;00m _text_to_bytes_and_warn,\n\u001b[0;32m     18\u001b[0m )\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mOpenSSL\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcrypto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     FILETYPE_PEM,\n\u001b[0;32m     21\u001b[0m     PKey,\n\u001b[0;32m     22\u001b[0m     X509,\n\u001b[0;32m     23\u001b[0m     X509Name,\n\u001b[0;32m     24\u001b[0m     X509Store,\n\u001b[0;32m     25\u001b[0m     _PassphraseHelper,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENSSL_VERSION_NUMBER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSLEAY_VERSION\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    126\u001b[0m ]\n\u001b[0;32m    129\u001b[0m OPENSSL_VERSION_NUMBER \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mOPENSSL_VERSION_NUMBER\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\OpenSSL\\crypto.py:1616\u001b[0m\n\u001b[0;32m   1612\u001b[0m         ext\u001b[38;5;241m.\u001b[39m_extension \u001b[38;5;241m=\u001b[39m _ffi\u001b[38;5;241m.\u001b[39mgc(extension, _lib\u001b[38;5;241m.\u001b[39mX509_EXTENSION_free)\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ext\n\u001b[1;32m-> 1616\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mX509StoreFlags\u001b[39;00m:\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;124;03m    Flags for X509 verification, used to change the behavior of\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;124;03m    :class:`X509Store`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1624\u001b[0m \u001b[38;5;124;03m        https://www.openssl.org/docs/manmaster/man3/X509_VERIFY_PARAM_set_flags.html\u001b[39;00m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1627\u001b[0m     CRL_CHECK: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_CRL_CHECK\n",
      "File \u001b[1;32mc:\\Users\\i.lutticken\\AppData\\Local\\anaconda3\\lib\\site-packages\\OpenSSL\\crypto.py:1635\u001b[0m, in \u001b[0;36mX509StoreFlags\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1633\u001b[0m EXPLICIT_POLICY: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_EXPLICIT_POLICY\n\u001b[0;32m   1634\u001b[0m INHIBIT_MAP: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_INHIBIT_MAP\n\u001b[1;32m-> 1635\u001b[0m NOTIFY_POLICY: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX509_V_FLAG_NOTIFY_POLICY\u001b[49m\n\u001b[0;32m   1636\u001b[0m CHECK_SS_SIGNATURE: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_CHECK_SS_SIGNATURE\n\u001b[0;32m   1637\u001b[0m PARTIAL_CHAIN: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m _lib\u001b[38;5;241m.\u001b[39mX509_V_FLAG_PARTIAL_CHAIN\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'lib' has no attribute 'X509_V_FLAG_NOTIFY_POLICY'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf   # Import the TensorFlow library, which provides tools for deep learning.\n",
    "import pandas as pd  # Import the pandas library, used for data manipulation and analysis.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Used for data management\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import tarfile\n",
    "\n",
    "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting and visualization.\n",
    "# This line allows for the display of plots directly within the Jupyter notebook interface.\n",
    "%matplotlib inline  \n",
    " \n",
    "# Import Keras libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Import the ImageDataGenerator class from Keras module in TensorFlow.\n",
    "from tensorflow.keras.models import Sequential  # Import the Sequential model: a linear stack of layers from Keras module in TensorFlow.\n",
    "from tensorflow.keras.layers import Dense  # Import the Dense layer: a fully connected neural network layer from Keras module in TensorFlow.\n",
    "from tensorflow.keras.layers import Flatten  # Import the Flatten layer: used to convert input data into a 1D array from Keras module in TensorFlow.\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy  # Import the SparseCategoricalCrossentropy loss function from Keras module in TensorFlow.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import losses\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the data\n",
    "\n",
    "**NOTE:** This notebook assumes that you have already downloaded the data as part of notebook 1 (*01_bees_vs_wasps.ipynb*). If you haven't, please go back and follow the instructions in that notebook to download the data.\n",
    "\n",
    "In the last notebook, Kevin noticed that his model had a big problem: it thought everything was wasps! Let's explore the data a bit and see if we can figure out why.\n",
    "\n",
    "First let's make a histogram of the number of images in each class. That should give us a good idea of how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign the bee_vs_wasp folder to the variable data_dir\n",
    "data_dir = r'data/bee_vs_wasp' # The 'r' at the beginning of the string indicates a raw string, which is used to prevent the characters in the string from being escaped.\n",
    "\n",
    "# Make a histogram of the number of images in each class\n",
    "def make_hist(data_dir):\n",
    "    # Get the class names\n",
    "    class_names = os.listdir(data_dir)\n",
    "\n",
    "    # Initialize an empty list to store the number of images in each class\n",
    "    num_images = []\n",
    "\n",
    "    # Loop through each class\n",
    "    for class_name in class_names:\n",
    "        # Get the list of images in the class\n",
    "        images = os.listdir(os.path.join(data_dir, class_name))\n",
    "        # Append the number of images in the class to the list\n",
    "        num_images.append(len(images))\n",
    "\n",
    "    # Put the number of images in each class in descending order\n",
    "    num_images, class_names = zip(*sorted(zip(num_images, class_names), reverse=True))\n",
    "\n",
    "    # Print the number of images in each class\n",
    "    for i in range(len(class_names)):\n",
    "        print(f'{class_names[i]}: {num_images[i]}')\n",
    "\n",
    "    print(\"************************************\")\n",
    "\n",
    "    # Print the percentage of images in each class\n",
    "    total_images = sum(num_images)\n",
    "    for i in range(len(class_names)):\n",
    "        percentage = (num_images[i] / total_images) * 100\n",
    "        print(f'{class_names[i]}: {percentage:.2f}%')\n",
    "        \n",
    "    # Create a histogram of the number of images in each class\n",
    "    plt.bar(class_names, num_images)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.title('Number of Images in Each Class')\n",
    "    plt.show()\n",
    "\n",
    "make_hist(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! That's a pretty big class imbalance. Large class imbalances tend to make models biased towards the majority class. This is likely why Kevin's model was predicting everything as wasps. Let's try to fix this.\n",
    "\n",
    "There are many ways to address this issue. One of which is stratified sampling. This is a technique where we sample from the dataset in such a way that the distribution of classes in the sample is the same as the distribution of classes in the dataset. This way, we can ensure that the model sees a balanced dataset during training.\n",
    "\n",
    "Let's try this out. We'll use the `train_test_split` function from `sklearn` to split the data into training and validation sets. We'll set the `stratify` parameter to the labels so that the function samples in a stratified manner.\n",
    "\n",
    "## 3. Stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2585040214.py, line 130)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 130\u001b[1;36m\u001b[0m\n\u001b[1;33m    X_train, X_val, train_generator, val_generator = prep_display_data(data_path, batch_size=32, shape=(80,80,3), show_pictures=True, stratify=True, normalize=True, rand_oversample=False\u001b[0m\n\u001b[1;37m                                                                                                                                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the data directory\n",
    "data_path = r'data/bee_vs_wasp' # The 'r' at the beginning of the string indicates a raw string, which is used to prevent the characters in the string from being escaped.\n",
    "\n",
    "# Create a function that loads data for an object detection model\n",
    "def prep_display_data(path, batch_size=32, shape=(80,80,3), show_pictures=True, stratify=True, normalize=True, rand_oversample=True):\n",
    "    '''Takes a path, batch size, target shape for images and optionally whether to show sample images.\n",
    "       Returns training and testing datasets\n",
    "    '''\n",
    "    print(\"***********************************************************************\")\n",
    "    print(\"Load data:\")\n",
    "    print(f\"  - Loading the dataset from: {path}.\")\n",
    "    print(f\"  - Using a batch size of: {batch_size}.\")\n",
    "    print(f\"  - Resizing input images to: {shape}.\")\n",
    "    print(\"***********************************************************************\")\n",
    "    \n",
    "    batch_size = batch_size  # Define the batch size\n",
    "    \n",
    "    # Define the image size using the 1st 2 elements of the shape parameter\n",
    "    # We don't need the number of channels here, just the dimensions to use\n",
    "    image_size = shape[:2]\n",
    "\n",
    "    # Get the class names\n",
    "    class_names = os.listdir(path)\n",
    "\n",
    "    images = [] # Initialize the images list\n",
    "    labels = [] # Initialize the labels list\n",
    "\n",
    "    # Get the images and labels to use for training and validation\n",
    "    for class_name in class_names:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        for image_name in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_name)\n",
    "            images.append(image_path)\n",
    "            labels.append(class_name)\n",
    "\n",
    "    # Print the number of number of images per class\n",
    "    for class_name in class_names:\n",
    "        print(f'Number of {class_name} images: {labels.count(class_name)}')\n",
    "\n",
    "    if rand_oversample:\n",
    "        # Get the class with the most images\n",
    "        max_class = max([labels.count(class_name) for class_name in class_names])\n",
    "        print(f\"Max class: {max_class}\")\n",
    "\n",
    "        # Oversample the other classes to match the class with the most images\n",
    "        for class_name in class_names:\n",
    "            class_path = os.path.join(path, class_name)\n",
    "            while labels.count(class_name) < max_class:\n",
    "                image_name = np.random.choice(os.listdir(class_path))\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                images.append(image_path)\n",
    "\n",
    "                labels.append(class_name)\n",
    "\n",
    "        # Print the number of number of images per class after oversampling\n",
    "        for class_name in class_names:\n",
    "            print(f'Number of {class_name} images after oversampling: {labels.count(class_name)}')\n",
    "\n",
    "    \n",
    "    if stratify: # Use sklearn's train_test_split function to split the data into training and testing sets\n",
    "        # Split the data in a stratified manner\n",
    "        X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, stratify=labels)\n",
    "    else:\n",
    "        # Split the data randomly\n",
    "        X_train, X_val = train_test_split(images, test_size=0.2)\n",
    "\n",
    "    # Build the DataFrames for the training and validation sets\n",
    "    train_df = pd.DataFrame(list(zip(X_train, y_train)), columns=['image', 'class'])\n",
    "    val_df = pd.DataFrame(list(zip(X_val, y_val)), columns=['image', 'class'])\n",
    "    \n",
    "    if normalize:\n",
    "        # Define the ImageDataGenerator class with rescaling for each channel\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "        val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator()\n",
    "        val_datagen = ImageDataGenerator()\n",
    "    \n",
    "    # Define the training and validation data generators\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='image',\n",
    "        y_col='class',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_dataframe(\n",
    "        dataframe=val_df,\n",
    "        x_col='image',\n",
    "        y_col='class',\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "\n",
    "    if show_pictures:\n",
    "        # Get the class names\n",
    "        class_names = list(train_generator.class_indices.keys())\n",
    "        print(class_names)\n",
    "\n",
    "        # Display up to 3 images from each of the categories\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            \n",
    "            # Get a single batch to use for display\n",
    "            images, labels = train_generator.next()\n",
    "\n",
    "            # Un-normalize the images for display if normalization was used\n",
    "            if normalize:\n",
    "                images = images * 255\n",
    "\n",
    "            # Filter images of the current class\n",
    "            class_images = images[labels[:, i] == 1]\n",
    "                \n",
    "            # Number of images to show. We don't want to show more than 3 images.\n",
    "            num_images = min(len(class_images), 3)\n",
    "                \n",
    "            for j in range(num_images):\n",
    "                ax = plt.subplot(1, num_images, j + 1)\n",
    "                plt.imshow(class_images[j].astype(\"uint8\"))\n",
    "                plt.title(class_name)\n",
    "                plt.axis(\"off\")\n",
    "            plt.show()\n",
    "\n",
    "    return X_train, X_val, train_generator, val_generator\n",
    "\n",
    "X_train, X_val, train_generator, val_generator = prep_display_data(data_path, batch_size=32, shape=(80,80,3), show_pictures=True, stratify=True, normalize=True, rand_oversample=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make our model\n",
    "\n",
    "This function creates the model we will use.\n",
    "\n",
    "One hyperparameter to explore is the activation function, which is set when making the model. We start with a ReLU as the default, but you can try others. For simplicity, we will use the same activation function for all but the last layer of the model, but you could change them individually.\n",
    "\n",
    "The last layer will almost always use a Softmax, which makes all the output values between 0 and 1 and sum to 1, transforming them into probabilities of the input belonging to each possible class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(activation='relu', shape=(80,80,3), num_classes=4):\n",
    "    '''Sets up a model. \n",
    "          Takes in an activation function, shape for the input images, and number of classes.\n",
    "          Returns the model.'''\n",
    "    print(\"***********************************************************************\")\n",
    "    print(\"Make model:\")\n",
    "    print(f\"  - Using the activation function: {activation}.\")\n",
    "    print(f\"  - Model will have {num_classes} classes.\")\n",
    "    print(\"***********************************************************************\")\n",
    "    \n",
    "    # Define the model\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=activation, input_shape=shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=activation),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation=activation),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation=activation),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compile and Train the model\n",
    "\n",
    "This step compiles the model, getting it ready for training. The primary hyperparameters here are:\n",
    "* the **loss function** (how we determine how close the predicted output is from the known output values),\n",
    "* the **optimization function** (how we determine what parameters should be updated and how),\n",
    "* the **learning rate** (how much each parameter should be adjusted), \n",
    "* and how many **epochs** should be run (remember, an epoch is a full pass through all the training data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define a function that takes an optimizer name as a string\n",
    "def load_optimizer(optimizer_name):\n",
    "  # Check if the optimizer name is valid\n",
    "  if optimizer_name in tf.keras.optimizers.__dict__:\n",
    "    # Return the corresponding optimizer function\n",
    "    return tf.keras.optimizers.__dict__[optimizer_name]\n",
    "  else:\n",
    "    # Raise an exception if the optimizer name is invalid\n",
    "    raise ValueError(f\"Invalid optimizer name: {optimizer_name}\")\n",
    "\n",
    "def compile_train_model(train_set, val_set, model,\n",
    "                        loss='categorical_crossentropy',\n",
    "                        optimizer='Adam', learning_rate=0.0001, epochs=10):\n",
    "    '''Compiles and trains the model. \n",
    "          Takes in an training set, a validation set, model, loss function, optimizer, learning rate,\n",
    "          and epochs.\n",
    "          Returns the compiled model and training history.'''\n",
    "    print(\"***********************************************************************\")\n",
    "    print(\"Compile and Train the model:\")\n",
    "    print(f\"  - Using the loss function: {loss}.\")\n",
    "    print(f\"  - Using the optimizer: {optimizer}.\")\n",
    "    print(f\"  - Using learning rate of: {learning_rate}.\")\n",
    "    print(f\"  - Running for {epochs} epochs.\")\n",
    "    print(\"***********************************************************************\")\n",
    "    # Compile the model\n",
    "    \n",
    "    opt= load_optimizer(optimizer)(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "    # Train the model\n",
    "    history = model.fit(train_set, epochs=epochs, validation_data=val_set)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "model, history = compile_train_model(train_generator, val_generator, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the model\n",
    "\n",
    "Now that we have trained our model let's evaluate how it does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(train_set, val_set, model, history, num_classes=4):\n",
    "    '''Evaluates a model. \n",
    "          Takes in a train_set, val_set, model, history, number of classes.'''\n",
    "    print(\"***********************************************************************\")\n",
    "    print(\"Evaluate the model:\")\n",
    "    print(\"***********************************************************************\")\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(train_set)\n",
    "    print(f'Test loss: {loss}')\n",
    "    print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "    # Plot the training and validation loss over time\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the training and validation accuracy over time\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    class_names = list(train_set.class_indices.keys()) # Get the class names\n",
    "    \n",
    "    y_pred = np.argmax(model.predict(val_set), axis=-1) # Make predictions on the test set\n",
    "\n",
    "    num_val_samples = val_set.samples  # total number of validation samples\n",
    "    batch_size = val_set.batch_size  # batch size\n",
    "\n",
    "    # Calculate the number of batches\n",
    "    num_batches = np.ceil(num_val_samples / batch_size)\n",
    "    num_batches = int(num_batches)\n",
    "\n",
    "    # Get the true labels\n",
    "    y_true = np.concatenate([y for x, y in (next(val_set) for _ in range(num_batches))], axis=0)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true.argmax(axis=1), y_pred)\n",
    "    \n",
    "    # Plot the confusion matrix\n",
    "    plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(range(num_classes),class_names)\n",
    "    plt.yticks(range(num_classes), class_names)\n",
    "    # plt.colorbar()\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "    plt.show()\n",
    "\n",
    "evaluate_model(train_generator, val_generator, model, history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore hyperparameters!\n",
    "\n",
    "OK, we've trained the model once using some decent first guesses. Now, we can see if we can do better by exploring different hyperparameters.\n",
    "\n",
    "While there are methods to explore different hyperparameters systematically and track the results more efficiently, we will rely on some ad-hoc exploration and keep everything in the notebook.\n",
    "\n",
    "The following function pulls all the steps from above into a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def the_whole_shebang(path, batch_size, shape, classes, activation, loss, optimizer, show_pictures=True):\n",
    "    \n",
    "    X_train, X_val, train_generator, val_generator = prep_display_data(data_path, batch_size, shape, show_pictures)\n",
    "    model = make_model(activation=activation, shape=shape, num_classes=classes)\n",
    "    model, history = compile_train_model(train_generator, val_generator, model, loss=loss,\n",
    "                        optimizer=optimizer, learning_rate=learning_rate, epochs=epochs)\n",
    "    evaluate_model(train_generator, val_generator, model, history, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the next cell and change hyperparameters\n",
    "\n",
    "You can copy the next cell multiple times and adjust the hyperparameters to compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data_path = 'data/bee_vs_wasp/' # Path to the data.\n",
    "        # This is defined above, you only need to change this if you change datasets\n",
    "    \n",
    "show_pictures = True # Show sample images from the dataset? Keep on at first, but may become distracting.\n",
    "                     # Set to False to turn off\n",
    "\n",
    "# Hyperparameters\n",
    "shape = (80,80,3)  # Dimensions to use for the images...the raw data are 80x80\n",
    "                   #  color images, but you could down-sample them\n",
    "                   #  or convert them to black and white if you wanted\n",
    "batch_size = 32  # What batch size to use\n",
    "classes = 4 # We have 4 classes in our dataset: bee, wasp, other_insect, other_noninsect\n",
    "            # Only change this if you change the dataset\n",
    "activation='relu' # The activation function is an important hyperparameter\n",
    "                  # Other activations functions to try: tanh, sigmoid\n",
    "\n",
    "loss= 'categorical_crossentropy' # Loss function\n",
    "        # Other loss functions to try: losses.CategoricalHinge()\n",
    "        #                              losses.KLDivergence()\n",
    "\n",
    "optimizer='Adagrad' # Optimizer: Adagrad is just an example, others to try are Adam or RMSprop\n",
    "\n",
    "learning_rate=0.001 # Try increasing or decreasing the learning rate by an order of magnitude\n",
    "\n",
    "epochs = 10 # Try running more epochs\n",
    "\n",
    "# Run everything with these hyperparameters\n",
    "the_whole_shebang(data_path, batch_size, shape, classes, activation, loss, optimizer, show_pictures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. EDIT OR REMOVE A look inside CNNs\n",
    "To get an idea for what is happening *inside* this model, let's look at a **feature map**. Below we see a vertical edge detection filter applied to a sunflower picture, resulting in a feature map of that image.\n",
    "\n",
    "![](images/01.1_filter_image.jpg)\n",
    "\n",
    "Imagine you're a detective investigating a scene.  A feature map is like a sketch you create, focusing on specific details that might be clues to solving the case.  In a CNN, the \"case\" is recognizing patterns in an image, and the feature maps capture these patterns at different levels of complexity. Early layers might create feature maps that detect basic edges, corners, or blobs of color. As the network progresses through more layers, the feature maps become more intricate, combining these simpler features to represent more complex objects or shapes.\n",
    "\n",
    "Getting a bit more technical, a feature map is a 2D array of activations produced by applying a convolutional filter to an input image or a previous layer's feature map. It essentially captures the presence and strength of specific visual features the filter is optimized to detect within the input.\n",
    "\n",
    "The **convolutional filters** (also just called \"filters\" or \"kernels\") are small matrices containing learnable weights. The filter \"slides\" across the input image, performing element-wise multiplication with the underlying image data at each position. The result of the multiplication is then passed through an activation function (like ReLU) to introduce non-linearity and help the network learn complex features. A convolutional layer typically has multiple filters, each generating a separate feature map. These feature maps capture different aspects of the input, providing a richer representation of the image.\n",
    "\n",
    "**NOTE**: The above sunflower example could potentially be a bit misleading. While a model *can* and probably will develop a vertical edge detection filter, the model develops it's filter's weights through the same backpropagation process as other deep neural networks. Most of the filters, and their resulting feature maps, will not be as easily interpretable as the vertical edge detection filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the filters from the first layer of the model\n",
    "filters = model.layers[0].get_weights()[0]\n",
    "\n",
    "# Get the first batch of images from the training set\n",
    "conv_images = X_train.take(1)\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in conv_images:\n",
    "    images = images.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "# Get the feature maps from the first layer of the model\n",
    "feature_maps = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
    "feature_maps = feature_maps.predict(images)\n",
    "\n",
    "# Normalize the filters and feature maps. This will make the images more clear.\n",
    "normal_filters = (filters - filters.min()) / (filters.max() - filters.min())\n",
    "normal_feature_maps = (feature_maps - feature_maps.min()) / (feature_maps.max() - feature_maps.min())\n",
    "\n",
    "# Display the filters, images and feature maps\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# ----- Filters -----\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, i + 1)  # 3 rows, 3 columns, position 1 to 3 \n",
    "    plt.imshow(normal_filters[:, :, :, i], cmap='gray')\n",
    "    plt.title(f'Filter {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "# ----- Original Images -----\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, i + 4)  # Position 4 to 6\n",
    "    plt.imshow(images[i].astype(\"uint8\"))\n",
    "    plt.title(f'Original Image {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "# -----  Feature Maps (Image 1) -----\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 3, i + 7)  # Position 7 to 9\n",
    "    plt.imshow(normal_feature_maps[1, :, :, i], cmap='gray')\n",
    "    plt.title(f'Feature Map 1, Channel {i}')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle('Visualizing a CNN')  # Overall title for the plot\n",
    "plt.tight_layout()  # Adjust spacing to prevent overlaps\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the other hyperparameters in Section 7 above, the number of filters, the size of the filters, and the stride of the filters are all hyperparameters that can be adjusted. You can also add or remove convolutional and pooling layers, or add dropout layers. Dropout layers are a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to 0 at each update during training. Here is an example of how to add a dropout layer to the model:\n",
    "    \n",
    "```python\n",
    "    # Import the Dropout layer\n",
    "    from tensorflow.keras.layers import Dropout\n",
    "\n",
    "    # Existing Conv2D layer\n",
    "    # Dropout layer with a 50% dropout rate\n",
    "    model.add(Dropout(0.5)) \n",
    "    # Existing MaxPooling2D layer\n",
    "```\n",
    "To adjust the stride and padding of the convolutional layers, you can add the `strides` and `padding` arguments to the `Conv2D` layer. The `strides` argument is a tuple of two integers, specifying the strides of the convolution along the height and width. The `padding` argument can be either `'valid'` or `'same'`. `'valid'` means no padding, while `'same'` means the output feature map will have the same spatial dimensions as the input feature map. Here is an example of a convulutional layer with a stride of 2 and padding of `'same'`:\n",
    "    \n",
    "```python\n",
    "    # Replacement Conv2D layer\n",
    "    layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation=activation)\n",
    "    # Replacement MaxPooling2D layer\n",
    "    layers.MaxPooling2D((2, 2), padding='same', strides=(2, 2))\n",
    "```\n",
    "\n",
    "## 9. Conclusion\n",
    "Experiment with the code in Section 4 to see how different hyperparameters and model architectures affect the model's performance. That's it. We'll see you in the next Module!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise\n",
    "If you found this exercise pretty simple, try editing the code in this notebook such that our function from Section 7 (*the_whole_shebang*) can control the Dropout rate, stride, and padding of the convolutional layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
